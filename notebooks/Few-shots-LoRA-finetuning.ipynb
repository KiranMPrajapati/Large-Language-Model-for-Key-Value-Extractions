{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48128e8f-c5b2-4fcd-a468-7dc93963efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08e937-2c31-4778-85c4-3fa81ba1aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='../data/dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2900a-f208-4117-98b5-4dfddcbfbdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"\"\"{example1}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a0f5e-3143-468f-be86-a6cb461db96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1 = \"\"\"{answers1}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeac0b-9ae3-4c5e-9cc2-867169a5d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example2 = \"\"\"{example2}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c820c5-3cb3-4f6d-8b0d-2c7827b48279",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer2 = \"\"\"{answer2}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314fddb-bb74-4d4f-81d3-6b8ca78dcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(data_point):\n",
    "    system_prompt = \"\"\"{Add question}\"\"\"\n",
    "    addon_prompt =  \"\"\"You are a {task}. {Add addon prompt}. \n",
    "                If the answer is not available, please explicitly state that the information is not known or accessible.\"\"\"\n",
    "    input_prompt =f\"\"\"{addon_prompt}\\n{system_prompt}\\n\n",
    "    ### USER: {data_point[\"INPUT\"]}\n",
    "    ### ASSISTANT: {data_point[\"RESPONSE \"]}\n",
    "    \"\"\"\n",
    "    return {\"text\": input_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b8c68-70dd-4489-80be-cffc8ccb9d6b",
   "metadata": {},
   "source": [
    "### Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfecc8-2da4-40bb-a84b-116121ba38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1c029-0a14-40cc-a333-3539fdb9652a",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c335323-6a94-46e4-97df-306dc018d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=1024,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030098d-15a0-4452-82dd-a32b6f1f1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55883e-bad9-4e39-bd20-b9282f6a03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    system_prompt = \"\"\"{Add question}\"\"\"\n",
    "    addon_prompt = \"\"\"You are a {task}. {Add addon prompt}. \n",
    "                If the answer is not available, please explicitly state that the information is not known or accessible.\"\"\"\n",
    "    full_prompt =f\"\"\"{addon_prompt}\\n{system_prompt}\\n\n",
    "    ### USER: {data_point[\"INPUT\"]}\n",
    "    ### ASSISTANT: {data_point[\"RESPONSE \"]}\n",
    "    \"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a37ecf-7834-4eaf-a80f-8d2ca3194860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepared = dataset['train'].train_test_split(test_size=0.1, shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f79bd3-bd0e-4fea-8453-625510cc62aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = dataset_prepared['train'].map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = dataset_prepared['test'].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad7073-06ff-4439-a2ff-48bbe529d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepared['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a98e5-9d35-4219-951d-8bbf40908138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prepared['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63222c06-bca5-4ea0-b45f-913fa1c95f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset[4]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d4caa-10d4-4192-9800-c545ea9bf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812b0ba-bb1d-4a24-a3b6-db7755fcb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_train_dataset[4]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9485d-27f9-4931-811f-8b80fb4f8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0de96-5dbb-4cc7-8155-47ce6ac4ec08",
   "metadata": {},
   "source": [
    "### Evaluation on base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff8b15-f3ae-42c2-83ad-69a3f0026b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"You are a question-answering system. Given the context answer the user queries.\n",
    "\n",
    "### USER: Text: Q:\\n\\nPosition character based on enemy coordinates in lua\\n\\nI have written a function here which should turn my character based on enemy coordinates but it\\'s not perfect because it does not always turn where I want it to and perhaps there is a better way of writing it\\nlocal myPosition = {x = 350, y = 355}\\nlocal enemyPosition = {x = 352, y = 354}\\nlocal xValue, yValue, xDir, yDir, dir\\n\\nif myPosition.x > enemyPosition.x then\\n    xValue = myPosition.x - enemyPosition.x\\nelseif myPosition.x < enemyPosition.x then\\n    xValue = myPosition.x - enemyPosition.x\\nelse\\n    xValue = 0\\nend\\n\\nif myPosition.y > enemyPosition.y then\\n    yValue = myPosition.y - enemyPosition.y\\nelseif myPosition.y < enemyPosition.y then\\n    yValue = myPosition.y - enemyPosition.y\\nelse\\n    yValue = 0\\nend\\n\\nif xValue < 0 then\\n    xDir = \"TURN RIGHT\"\\nelseif xValue > 0 then\\n    xDir = \"TURN LEFT\"\\nend\\n\\nif yValue < 0 then\\n    yDir = \"TURN DOWN\"\\nelseif yValue > 0 then\\n    yDir = \"TURN UP\"\\nend\\n\\nif xValue > yValue then\\n    dir = xDir\\nelseif xValue \\n    dir = yDir\\nend\\n\\nprint(\"Turn: \" .. dir)\\n\\nAnd here you have some pictures to further illustrate what I have in mind:\\n\\nAs you can see on the pictures, direction depends on the higher number.\n",
    "\n",
    "### ASSISTANT: I've read this text.\n",
    "\n",
    "### USER: What describes programming concept in the text?\n",
    "\n",
    "### ASSISTANT: \n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfee19-6226-4dd2-a526-817e5f7a7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65efa57f-f20c-452b-906d-130ec3192401",
   "metadata": {},
   "source": [
    "### Setup Lora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c3653-1326-480f-9421-b9823284f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89960b6-b6cd-45d0-9a1e-20bdbc8c4a8c",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e0d56-43f2-4ecf-9709-0b4e95e96a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9d4926-3638-4394-8446-3744495f3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "project = \"viggo-finetune1\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b307d1-39c9-4f99-94a2-9f271d903bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d735063e-f040-4605-94f0-f9569fda62e7",
   "metadata": {},
   "source": [
    "### Try the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85238b-5caa-4799-bc8f-7fdb4a60b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-viggo-finetune/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9bdf0-9998-4384-8dec-94d2e9f809dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"You are a question-answering system. Given the context answer the user queries.\n",
    "\n",
    "### USER: Text: Q:\\n\\nPosition character based on enemy coordinates in lua\\n\\nI have written a function here which should turn my character based on enemy coordinates but it\\'s not perfect because it does not always turn where I want it to and perhaps there is a better way of writing it\\nlocal myPosition = {x = 350, y = 355}\\nlocal enemyPosition = {x = 352, y = 354}\\nlocal xValue, yValue, xDir, yDir, dir\\n\\nif myPosition.x > enemyPosition.x then\\n    xValue = myPosition.x - enemyPosition.x\\nelseif myPosition.x < enemyPosition.x then\\n    xValue = myPosition.x - enemyPosition.x\\nelse\\n    xValue = 0\\nend\\n\\nif myPosition.y > enemyPosition.y then\\n    yValue = myPosition.y - enemyPosition.y\\nelseif myPosition.y < enemyPosition.y then\\n    yValue = myPosition.y - enemyPosition.y\\nelse\\n    yValue = 0\\nend\\n\\nif xValue < 0 then\\n    xDir = \"TURN RIGHT\"\\nelseif xValue > 0 then\\n    xDir = \"TURN LEFT\"\\nend\\n\\nif yValue < 0 then\\n    yDir = \"TURN DOWN\"\\nelseif yValue > 0 then\\n    yDir = \"TURN UP\"\\nend\\n\\nif xValue > yValue then\\n    dir = xDir\\nelseif xValue \\n    dir = yDir\\nend\\n\\nprint(\"Turn: \" .. dir)\\n\\nAnd here you have some pictures to further illustrate what I have in mind:\\n\\nAs you can see on the pictures, direction depends on the higher number.\n",
    "\n",
    "### ASSISTANT: I've read this text.\n",
    "\n",
    "### USER: What describes programming concept in the text?\n",
    "\n",
    "### ASSISTANT: \n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
