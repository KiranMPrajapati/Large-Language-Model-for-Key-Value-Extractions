{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/gime-extract/miniconda3/envs/llmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 3.65MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 3.08MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:01<00:00, 1.53MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 156kB/s]\n",
      "config.json: 100%|██████████| 720/720 [00:00<00:00, 3.11MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 92.7k/92.7k [00:00<00:00, 459kB/s]\n",
      "model-00001-of-00019.safetensors: 100%|██████████| 4.89G/4.89G [24:14<00:00, 3.36MB/s]\n",
      "model-00002-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [29:02<00:00, 2.86MB/s]\n",
      "model-00003-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [29:59<00:00, 2.77MB/s]\n",
      "model-00004-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [17:02<00:00, 4.79MB/s]\n",
      "model-00005-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [15:01<00:00, 5.53MB/s]\n",
      "Downloading shards:  26%|██▋       | 5/19 [1:55:24<4:50:09, 1243.55s/it]Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/56/dd/56dddba94664e0d83f2b33a201ebd592ec49ebaefd116e268abfb7cc19cbdf5c/048fa5347877b6d04eccf69765d23e5561cc9820dd4d0e5ba2df0100204dfb04?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27model-00006-of-00019.safetensors%3B+filename%3D%22model-00006-of-00019.safetensors%22%3B&Expires=1711097260&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMTA5NzI2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU2L2RkLzU2ZGRkYmE5NDY2NGUwZDgzZjJiMzNhMjAxZWJkNTkyZWM0OWViYWVmZDExNmUyNjhhYmZiN2NjMTljYmRmNWMvMDQ4ZmE1MzQ3ODc3YjZkMDRlY2NmNjk3NjVkMjNlNTU2MWNjOTgyMGRkNGQwZTViYTJkZjAxMDAyMDRkZmIwND9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=VUmq5ISXRPLz6rp4GZ%7EGgnCm31p8G7nvJ9pbhSO%7EdQHBgbgE%7EzI64vhpNPQ0fIEkhlcnhS1jLVi6usaJOploZ6XANn4fFyheSV5atZJV7b4LUDcLsGy3hmKLUV6%7EiLl1vBmP%7E2hH1kVhUhFrCwnBIbPF62278h287II%7EIArPbdj077%7Ekwu0r20vu7ZcOpLd2bkyGJhLOAAB%7EDgpCZzyG2n0htbatWnHqD5k5nstXGcsYc4hbLm-QUScbwdRXHdQooO9rdhWU53z42tYZTRuYperI%7E4Bm2SauOF3MngFhrrze%7EdAH3H7DOyogMJD8Ppf--kLXQ9osrnJWOrrTPilO1w__&Key-Pair-Id=KCD77M1F0VK2B: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model-00006-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [10:25<00:00, 5.57MB/s]\n",
      "model-00006-of-00019.safetensors:  30%|███       | 1.50G/4.98G [15:07<35:08, 1.65MB/s]\n",
      "model-00007-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [14:37<00:00, 5.58MB/s]\n",
      "model-00008-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [14:52<00:00, 5.58MB/s]\n",
      "model-00009-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [14:55<00:00, 5.56MB/s]\n",
      "model-00010-of-00019.safetensors: 100%|██████████| 4.90G/4.90G [18:04<00:00, 4.52MB/s]\n",
      "model-00011-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [15:00<00:00, 5.53MB/s]\n",
      "model-00012-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [14:57<00:00, 5.55MB/s]\n",
      "model-00013-of-00019.safetensors: 100%|██████████| 4.98G/4.98G [15:00<00:00, 5.53MB/s]\n",
      "Downloading shards:  68%|██████▊   | 13/19 [3:58:06<1:33:29, 934.99s/it]"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 2.55MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 3.10MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:01<00:00, 1.74MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 145kB/s]\n",
      "config.json: 100%|██████████| 720/720 [00:00<00:00, 1.34MB/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mixtral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mixtral-8x7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> 6\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<snippet> \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m risk-free rate of return ( 2 )\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 1.5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m ( 1.5 \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 1.5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m ( 1.5 \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 1.3\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m ( 1.3 \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m )\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m </snippet> \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m     On the scale of 1 to 5, how relevant is the above snippet to the following question: \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you have mayonnaise recipes?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     42\u001b[0m ]\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 526\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m-> 1064\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llmenv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    762\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    763\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mixtral'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"<snippet> \" 3\",\" risk-free rate of return ( 2 )\",\" 1.5% ( 1.5 % )\",\" 1.5% ( 1.5 % )\",\" 1.3% ( 1.3 % )\" </snippet> \n",
    "     On the scale of 1 to 5, how relevant is the above snippet to the following question: \n",
    "     <question> What is risk-free rate of return ( 2 ) for 2015 ? </question>\n",
    "     Consider the following examples and their relevance scores:\n",
    "     Example 1:\n",
    "     <snippet> \" 6\",\" fair value per option granted\",\" $ 15.01\",\" $ 12.04\",\" $ 11.03\" </snippet> \n",
    "     On the scale of 1 to 5, how relevant is the above snippet to the following question: \n",
    "     <question> What is credit funds2 for unfunded commitments ? </question>\n",
    "     Relevance score: 1\n",
    "     Example 2: \n",
    "     <snippet> \" 3\",\" 2011\",\" 3139\",\" 91\",\" 3048\" </snippet> \n",
    "     On the scale of 1 to 5, how relevant is the above snippet to the following question: \n",
    "     <question> What is 2010 for benefit payments ? </question>\n",
    "     Relevance score: 2\n",
    "     Example 3: \n",
    "     <snippet> \" 2\",\" private equity funds2\",\" $ 7680\",\" $ 2778\",\" $ 8074\",\" $ 3514\" </snippet> \n",
    "     On the scale of 1 to 5, how relevant is the above snippet to the following question: \n",
    "     <question> What is credit funds2 for unfunded commitments ? </question>\n",
    "     Relevance score: 3\n",
    "     Example 4: \n",
    "     <snippet> \" 3\",\" credit funds2\",\" 3927\",\" 2843\",\" 3596\",\" 3568\" </snippet> \n",
    "     On the scale of 1 to 5, how relevant is the above snippet to the following question: \n",
    "     <question> What is credit funds2 for unfunded commitments ? </question>\n",
    "     Relevance score: 4\n",
    "     \n",
    "     Focus on the snippet's relevance to the main topic and details mentioned in the question.\n",
    "     \n",
    "     Carefully consider the relevance of the snippet to the question, and provide a score from 1 to 4, where 1 is least relevant and 4 is the most relevant. Use the full range of scores to indicate the degree of relevance.\n",
    "     \n",
    "     Respond only with number.\n",
    "     \"\"\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = embed_model.generate(inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/pseudo_json_dataset_V3_2048.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 2010 for benefit payments ?\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "print(data.loc[index, 'question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data.loc[index, 'html_content'], 'html.parser')\n",
    "\n",
    "# Find the table\n",
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(columns=['index', 'question', 'csv_row', 'cosine_similarity', 'dot_product_similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:01, 12.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [03:05, 10.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# with open(filename, 'w') as file:\n",
    "count = 0\n",
    "for i, row in tqdm(data[:2000].iterrows()):\n",
    "    soup = BeautifulSoup(row['html_content'], 'html.parser')\n",
    "\n",
    "    # Find the table\n",
    "    table = soup.find('table')\n",
    "    # Open StringIO object for writing\n",
    "    csv_buffer = StringIO()\n",
    "\n",
    "\n",
    "    trows = table.find_all('tr')\n",
    "    header = trows[0]\n",
    "    question_embed = embed_model.encode(row['question'])\n",
    "\n",
    "    for trow in trows[1:]:\n",
    "        writer = csv.writer(csv_buffer)\n",
    "        writer.writerow([cell.get_text() for cell in trow.find_all(['td', 'th'])])\n",
    "        csv_data = csv_buffer.getvalue().replace('\\n', '').replace('  ', '')\n",
    "        csv_data_embed = embed_model.encode(csv_data)\n",
    "        csv_buffer = StringIO()\n",
    "        sim = np.dot(csv_data_embed, question_embed)\n",
    "        cosine_sim = cosine_similarity(csv_data_embed.reshape(1, -1), question_embed.reshape(1, -1))[0][0]\n",
    "        output_df.at[count, 'question'] = row['question']\n",
    "        output_df.at[count, 'cosine_similarity'] = cosine_sim\n",
    "        output_df.at[count, 'dot_product_similarity'] = sim\n",
    "        output_df.at[count, 'csv_row'] = csv_data\n",
    "        output_df.at[count, 'index'] = row['index']\n",
    "        count+=1\n",
    "        \n",
    "        # writer.writerow([cell.get_text() for cell in trow.find_all(['td', 'th'])])\n",
    "\n",
    "\n",
    "    # Get the CSV data as a string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_df.to_csv('../embedding_similarity/sentence_transformers/bert_base_nli_mean_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'RUCKBReasoning/TableLLM-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11719"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# with open(filename, 'w') as file:\n",
    "count = 0\n",
    "for i, row in tqdm(data[:2000].iterrows()):\n",
    "    soup = BeautifulSoup(row['html_content'], 'html.parser')\n",
    "\n",
    "    # Find the table\n",
    "    table = soup.find('table')\n",
    "    # Open StringIO object for writing\n",
    "    csv_buffer = StringIO()\n",
    "\n",
    "\n",
    "    trows = table.find_all('tr')\n",
    "    header = trows[0]\n",
    "    question_embed = tokenizer.encode(row['question'])\n",
    "\n",
    "    for trow in trows[1:]:\n",
    "        writer = csv.writer(csv_buffer)\n",
    "        writer.writerow([cell.get_text() for cell in trow.find_all(['td', 'th'])])\n",
    "        csv_data = csv_buffer.getvalue()\n",
    "        csv_data_embed = tokenizer.encode(csv_data)\n",
    "        csv_buffer = StringIO()\n",
    "        sim = np.dot(csv_data_embed, question_embed)\n",
    "        cosine_sim = cosine_similarity(csv_data_embed.reshape(1, -1), question_embed.reshape(1, -1))[0][0]\n",
    "        output_df.at[count, 'question'] = row['question']\n",
    "        output_df.at[count, 'cosine_similarity'] = cosine_sim\n",
    "        output_df.at[count, 'dot_product_similarity'] = sim\n",
    "        output_df.at[count, 'csv_row'] = csv_data.replace('\\n', '').replace('  ', '')\n",
    "        output_df.at[count, 'index'] = row['index']\n",
    "        count+=1\n",
    "        # writer.writerow([cell.get_text() for cell in trow.find_all(['td', 'th'])])\n",
    "\n",
    "\n",
    "    # Get the CSV data as a string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of the same values in both lists: 4\n"
     ]
    }
   ],
   "source": [
    "a = [2, 3, 2, 56, 78, 99]\n",
    "b = [3, 44, 2, 2, 5, 99, 22, 33, 44, 66, 88, 99]\n",
    "\n",
    "# Count occurrences of elements in list a\n",
    "a_counts = {}\n",
    "for item in a:\n",
    "    a_counts[item] = a_counts.get(item, 0) + 1\n",
    "\n",
    "# Count occurrences of elements in list b\n",
    "b_counts = {}\n",
    "for item in b:\n",
    "    b_counts[item] = b_counts.get(item, 0) + 1\n",
    "\n",
    "# Find common elements and count their occurrences\n",
    "common_values_count = 0\n",
    "for item, count in a_counts.items():\n",
    "    if item in b_counts:\n",
    "        common_values_count += min(count, b_counts[item])\n",
    "\n",
    "print(\"Count of the same values in both lists:\", common_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
